# Pinball highscore

By Per Isacson and Kim Tennfors

This small project is the result of a hacker day at Trustly. We have a pinball game and we want to be able to publish a new highscore on our big screens so that all people can see it - not just the ones who happen to stop by the pinball game.

To achieve that we need to 
1. take photos of the pinball game's display where the highscore is shown 
2. extract the text from the screen
3. interpret the text - the highscore is not shown all the time
4. publish the result somewhere

So far we have only built the optical character recognition part and that is what is included in this python project. Our source data was a couple of photos made with an iPhone of the pinball game's display. We processed these photos with OpenCV to get a black and white picture and extract the areas where a character was displayed. These characters where then input to a scikit-learn LogisticRegression model to train it to predict photos it had not seen before.

## Prerequisits to run the package
* OpenCV with python wrappers
* scikit-learn
* some photos from the pinball game to train the model on (included in the examples folder)

## Contents of this package
### label.py
This script takes a photo from the game's display as input and asks the user to label each character it finds in the photo. Once all characters are label, you can choose to store the result to a json-file that will contain each "training example" (a 4800 digit long numpy array with each pixel of the character) and an accompanying label. We had to choose a size for the characters and the resolutioin of our photos made 60x80 (=4800) a good choice. These values can be changed for other setups but they need to be consistent over training and prediction for the model to handle them.

Arguments to the script:
1. Image file to read
2. (optional) json file to store the data. Note that if the file exists, the new data will be added to it so you are expected to run the script once for each image file to build the training set.

Example: python label.py examples/img1.png data.json
(and then again for each photo: examples/img2.png, examples/img3.png, ...)

### train.py
This script loads the json file with training examples generated from label.py. As it is configured now it splits the data set into two: 33% for test/validation and 67% for training. It then trains the sklearn.linear_model.LogisticRegression model on the train data and does a prediction of the test data. A report on how well it did is logged to the terminal. A label encoder (sklearn.preprocessing.LabelEncoder) is used to map between the actual characters, e.g. 'G', 'H', '8', and the values input to/output from the model that have to be numeric. It is possible to store (pickle) the classifier and label encoder so that it can be loaded and used by the predict.py script.

Arguments to the script:
1. json file with train data generated by label.py
2. prefix for generating the pickled classifier and label encoder files

Example: python train.py data.json logistic
Will generate two files: logistic_model.pickle and logistic_le.pickle

### predict.py
This script loads a (preferrably) previously unseen photo of the pinball game's display together with the pickled model and label encoder generated by train.py. It then scans the photo for characters, shows the photo with a box around each character that it found and outputs the characters to the terminal.

Arguments to the script:
1. Image file to read and make prediction on
2. Prefix for the model and le pickle files generated by train.py

Example: python predict.py examples/img7.png logistic
Will load img7.png and the files logistic_model.pickle, logistic_le.pickle

## How well does it perform
We used a quite small training set (~60-80 characters). It seems to do a decent job on characters it has seen at least once before (we can't really expect it to recognize something it hasn't been trained on at least once!). There are a number of improvements we could try:
* Improve the training data. Either by adding more training examples or experimenting with how to preprocess the original photos to get distinct contours of the characters.
* Try a different model. Perhaps a Support Vecore Classifier would do a better job but it seems more obvious in this case to improve the training data.
* Use external training data. There are a number of character sets to be found (e.g. MNIST) that can be used to train OCR models.

## Where to go from here
Right now we can only parse characters from a photo of the "right" size. What happens when we get a new pinball game? How will we capture photos continuously - webcam? There are a number of issues left to think about but at least we could prove that with a moderate effort, it is possible to do OCR on photos of a pinball display and get ok results.
